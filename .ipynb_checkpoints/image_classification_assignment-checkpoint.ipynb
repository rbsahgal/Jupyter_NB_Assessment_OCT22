{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ef8c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "517d70d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset details \n",
    "image_height = 150\n",
    "image_width = 150\n",
    "number_of_channels = 3\n",
    "number_of_classes = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b084025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper-Parameters\n",
    "batch_size = 64\n",
    "NUMBER_OF_EPOCHS = 12\n",
    "LEARNING_RATE = 0.0007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b950ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############-------- START --------############\n",
      "This code run on on Python version :  3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]\n",
      "The Tensorflow version used is : 2.10.0\n"
     ]
    }
   ],
   "source": [
    "print(\"############-------- START --------############\")\n",
    "print(\"This code run on on Python version : \", sys.version)\n",
    "# Print statement check tensorflow is running\n",
    "print(\"The Tensorflow version used is : \" + tf. __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a51b6117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14034 files belonging to 6 classes.\n",
      "Found 3000 files belonging to 6 classes.\n",
      "['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n",
      "['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n"
     ]
    }
   ],
   "source": [
    "# Load Image Data from local computer\n",
    "# Use seg_train folder for training\n",
    "train_set = tf.keras.preprocessing.image_dataset_from_directory(r\"E:\\Work\\vs_code\\Assessment_Oct22\\SceneryDataset\\seg_train\",\n",
    "color_mode= \"rgb\",\n",
    "batch_size=batch_size,\n",
    "image_size=(image_height, image_width),\n",
    "shuffle=True,\n",
    "seed=123)\n",
    "\n",
    "# Use seg_test folder for hold-out validation\n",
    "validation_set = tf.keras.preprocessing.image_dataset_from_directory(r\"E:\\Work\\vs_code\\Assessment_Oct22\\SceneryDataset\\seg_test\",\n",
    "color_mode= \"rgb\",\n",
    "batch_size=batch_size,\n",
    "image_size=(image_height, image_width),\n",
    "shuffle=True,\n",
    "seed=123)\n",
    "\n",
    "print(train_set.class_names)\n",
    "print(validation_set.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdbc12c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "(128, 150, 150, 3)\n",
      "(128,)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "EagerTensor object has no attribute 'data'. \n        If you are looking for numpy-related methods, please run the following:\n        from tensorflow.python.ops.numpy_ops import np_config\n        np_config.enable_numpy_behavior()\n      ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m   \u001b[38;5;28mprint\u001b[39m(labels_batch\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     20\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlabels_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m)\n\u001b[0;32m     22\u001b[0m train_set \u001b[38;5;241m=\u001b[39m train_set\u001b[38;5;241m.\u001b[39mcache()\u001b[38;5;241m.\u001b[39mprefetch(buffer_size\u001b[38;5;241m=\u001b[39mAUTOTUNE)\n\u001b[0;32m     23\u001b[0m validation_set \u001b[38;5;241m=\u001b[39m validation_set\u001b[38;5;241m.\u001b[39mcache()\u001b[38;5;241m.\u001b[39mprefetch(buffer_size\u001b[38;5;241m=\u001b[39mAUTOTUNE)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:440\u001b[0m, in \u001b[0;36mTensor.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[0;32m    437\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mravel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranspose\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreshape\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    438\u001b[0m               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtolist\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;66;03m# TODO(wangpeng): Export the enable_numpy_behavior knob\u001b[39;00m\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    441\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;124m      If you are looking for numpy-related methods, please run the following:\u001b[39m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;124m      from tensorflow.python.ops.numpy_ops import np_config\u001b[39m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;124m      np_config.enable_numpy_behavior()\u001b[39m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m    446\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: EagerTensor object has no attribute 'data'. \n        If you are looking for numpy-related methods, please run the following:\n        from tensorflow.python.ops.numpy_ops import np_config\n        np_config.enable_numpy_behavior()\n      "
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Data augmentation - options - flipped left-right as mirror,introduced perturbations in contrast and brightness and rotated by delta\n",
    "image_augmentation = keras.Sequential(\n",
    "    [       \n",
    "        layers.RandomFlip(mode=\"horizontal\",\n",
    "                        input_shape=(image_height,\n",
    "                                    image_width,\n",
    "                                    3)),\n",
    "        layers.RandomContrast(factor=0.1,),\n",
    "        layers.RandomBrightness(factor=0.15),\n",
    "        layers.RandomRotation(0.1)\n",
    "    ]\n",
    ")\n",
    "class_names = train_set.class_names\n",
    "for image_batch, labels_batch in train_set:\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break\n",
    "\n",
    "train_set = train_set.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "validation_set = validation_set.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Model creation - this model includes dropout regularization\n",
    "model = tf.keras.Sequential([\n",
    "  image_augmentation,\n",
    "  tf.keras.layers.Rescaling(1./255),\n",
    "  tf.keras.layers.Conv2D(32, 3,padding='same', activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 3,padding='same', activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(64, 3,padding='same', activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(64, 3,padding='same', activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  layers.Dropout(0.2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(number_of_classes)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54694a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation - continued...\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "# Learning rate scheduler - regularization\n",
    "def lr_scheduler(epoch, lr):\n",
    "  if epoch < 3:\n",
    "    return lr\n",
    "  else:\n",
    "    return lr * 0.99\n",
    "\n",
    "# callbacks for training and regularisation\n",
    "lr_scheduler_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
    "\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dc4220",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "  train_set,\n",
    "  validation_data=validation_set,\n",
    "  epochs=NUMBER_OF_EPOCHS,\n",
    "  callbacks=[lr_scheduler_callback]\n",
    ")\n",
    "\n",
    "model.evaluate(validation_set, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80784c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track training progress\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(NUMBER_OF_EPOCHS)\n",
    "\n",
    "# Plot training and validation graphs\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29db23aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on new images\n",
    "pred_img = tf.keras.utils.load_img(\n",
    "    r\"E:\\Work\\vs_code\\Assessment_Oct22\\SceneryDataset\\seg_pred\\9992.jpg\", target_size=(image_height, image_width), color_mode=\"rgb\"\n",
    ")\n",
    "plt.figure(2,figsize=(5, 5))\n",
    "plt.imshow(pred_img)\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(pred_img)\n",
    "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "prediction_list = model.predict(img_array) \n",
    "prediction = prediction_list[0]\n",
    "score = tf.nn.softmax(prediction_list[0])\n",
    "predictionIndex = np.argmax(prediction)\n",
    "predictedClass = class_names[predictionIndex]\n",
    "prediction_confidence = round (100 * np.max(prediction), 2)\n",
    "\n",
    "for prediction_vals in prediction_list:\n",
    "    print(prediction_vals)\n",
    "    print(\"/n\")\n",
    "\n",
    "title_string  = \"This image is {} with a {:.2f} % confidence\". format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    "plt.title(title_string)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Save model to local computer\n",
    "#model.save('saved_model/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
